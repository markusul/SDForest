---
title: "SDTree"
output: rmarkdown::html_vignette
bibliography: REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{SDTree}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In the following, we compare the Spectrally Deconfounded Regression Tree $f^{SDTree}(X)$ to the classical regression tree $f^{Tree}(X)$ [@Breiman2017ClassificationTrees] using simulated data from a confounded process. We simulate the data similar to `simulate_data_nonlinear()` but with a random regression tree, i.e. step-function for the direct relationship $f(X)$.

```{r sim}
library(SDForest)

set.seed(99)

# number of confounding covariates in H
q <- 6

# number of covariates in X
p <- 500

# number of observations
n <- 500

# number of non-zero coefficients in beta
m <- 5
dat <- simulate_data_step(q, p, n, m, make_tree = TRUE)
```

Given the data, we can estimate both the SDTree and the classical regression tree. Here we estimate both trees with the default parameters. One could choose, for example, the regularization parameter cp in a more principled way than just setting it to 0.01, e.g. `cvSDTree()` or `regPath.SDTree()`. But 0.01 seem like a good first try and is also the default value in [rpart](https://cran.r-project.org/web/packages/rpart/index.html) [@Therneau2022Rpart:Trees].

```{r fit}
dec_tree <- SDTree(x = dat$X, y = dat$Y, cp = 0.01)
plain_tree <- SDTree(x = dat$X, y = dat$Y, Q_type = 'no_deconfounding', cp = 0.01)
```

Let us observe, what the different trees predict, by comparing the predictions to the true function and the observed response. On top, we show the underlying true step function $f^0(X)$ against the 5 covariates used for splitting partitions. On the bottom, we see the observed response $Y$. The SDTree $f^{SDTree}(X)$ predicts almost the same response as the true underlying function. The plain classical regression tree, on the other hand, results in a prediction close to the observed $\mathbf{Y}$.

```{r pred, echo=FALSE, message=FALSE}
library(tidyr)
library(ggplot2)

f_X_tree <- plain_tree$predictions
f_X_tree_deconfounded <- dec_tree$predictions

# gather results
results <- data.frame(Y = dat$Y, f_X = dat$f_X, f_X_tree = f_X_tree,
                      f_X_tree_deconfounded = f_X_tree_deconfounded)

X_j  <- dat$X[, dat$j]
colnames(X_j) <- dat$j

results <- cbind(results, X_j)

results <- gather(results, key = "method", value = "pred", Y, f_X, 
    f_X_tree, f_X_tree_deconfounded)
results <- gather(results, key = 'j', value = 'X', -method, -pred)

results$method <- as.factor(results$method)
levels(results$method) <- c('f^0*(X)', 'f^Tree*(X)', 'f^SDTree*(X)', 'Y')
results$method <- factor(results$method, levels = c('f^0*(X)', 'f^SDTree*(X)', 'f^Tree*(X)', 'Y'))


gg_res <- ggplot(results, aes(x = X, y = pred)) +
  geom_point(size = 0.05) +
  facet_grid(method~j, labeller = label_parsed) +
  theme_bw() + 
  labs(x = expression(X[j]), y = expression(widehat(f(X))))

gg_res
```

Now, we can directly look at the estimated tree structures. On top is the true underlying random regression tree, and below are the two tree estimates. In the middle is the SDTree and on the bottom is the plain classical regression tree. The nodes that choose a covariate for a split, that is also used in the true underlying causal function, are marked in <span style="color: #b0193c;">red</span>. Besides $X_{320}$, the SDTree uses all the parents of $Y$ for splits. Since $X_{320}$ only splits three observations into the node with a response level of 49.6, it seems reasonable that the noise and confounding in the observed data overshadow this effect. The order of the splits in the SDTree is partially different from the true function. $X_{369}$ is used too early resulting in having to split with $X_{250}$ and $X_{89}$ in both following subtrees. Estimating the partitions using no deconfounding results in the regression tree on the bottom. Here, we only find two of the parents of $Y$. Especially problematic is that the not deconfounded regression tree first splits with a few irrelevant covariates. Therefore, if an observation has $X_{486} > 0.07$ or $X_{430} \leq -2.66$, which has no meaning in the underlying function, no relevant split is left in the sub-tree.

```{r plot_tree,  echo=FALSE, message=FALSE, warning=FALSE}
library(igraph)
library(data.tree)
labels <- c(tree = expression(f^0*(X)), tree_model = expression(f^Tree*(X)), 
            deconfounded_tree_model = expression(f^SDTree*(X)))


splitt_names <- function(node){
    node$name <- paste('X', node$j, ' <= ', round(node$s, 3), sep = '')
}

leave_names <- function(node){
    new_name <- as.character(round(node$value, 1))
    if(new_name %in% node$Get('name', filterFun = isLeaf)){
        new_name <- paste(new_name, '')
    }
    node$name <- new_name
}

tree <- dat$tree
tree$Do(splitt_names, filterFun = isNotLeaf)
tree$Do(leave_names, filterFun = isLeaf)
data_tree <- as.igraph.Node(dat$tree, directed = T)

get_node_j <- function(node){
    node_name <- names(node)
    node_name <- unlist(strsplit(node_name, split = 'X\\.|X| |<|=|>'))
    if(length(node_name) > 1){
        return(as.numeric(node_name[2]))
    }else {
        return(0)
    }
}

get_labels <- function(tree, rpart_tree = F){
    node_mask <- c()
    for(i in 1:length(tree)){
        node_mask <- append(node_mask, get_node_j(tree[[i]]))
    }
    node_shape <- rep('none', length(node_mask))
    node_shape[node_mask == 0] <- 'circle'
    vertex_label_color <- rep('black', length(node_mask))
    vertex_label_color[node_mask %in% dat$j] <- '#b0193c'
    arrow_lable <- rep(c('yes', 'no'), length(node_mask) - 1)
    return(list(node_shape = node_shape, vertex_label_color = vertex_label_color, arrow_lable = arrow_lable))
}

#layout(matrix(c(1, 1, 2, 3), 2, 2, byrow = TRUE))
par(mai = 0.25*c(1, 1, 1, 1))

tree_labels <- get_labels(data_tree)
plot(data_tree, layout = layout.reingold.tilford(data_tree, root = 1), vertex.shape = tree_labels$node_shape, 
     vertex.label.color = tree_labels$vertex_label_color, vertex.size = 25, vertex.color = 'lightblue', edge.arrow.size = 0.5, 
     edge.label = tree_labels$arrow_lable, edge.label.color = '#000000', edge.label.cex = 0.8, main = expression(f^0*(X)))

# plain tree
tree <- dec_tree$tree
tree$Do(splitt_names, filterFun = isNotLeaf)
tree$Do(leave_names, filterFun = isLeaf)
deconfounded_tree <- as.igraph.Node(tree, directed = T)

tree_labels <- get_labels(deconfounded_tree)
plot(deconfounded_tree, layout = layout.reingold.tilford(deconfounded_tree, root = 1), vertex.shape = tree_labels$node_shape, 
     vertex.label.color = tree_labels$vertex_label_color, vertex.size = 25, vertex.color = 'lightblue', edge.arrow.size = 0.5, 
     edge.label = tree_labels$arrow_lable, edge.label.color = 'black', edge.label.cex = 0.8, main = labels$deconfounded_tree_model)


splitt_names <- function(node){
    node$name <- paste('X', node$j, ' <= ', round(node$s, 2), sep = '')
}

par(mar = c(0, 0, 2, 0))
tree <- plain_tree$tree
tree$Do(splitt_names, filterFun = isNotLeaf)
tree$Do(leave_names, filterFun = isLeaf)
tree <- as.igraph.Node(tree, directed = T)
tree_labels <- get_labels(tree)
plot(tree, layout = layout.reingold.tilford(tree, root = 1), vertex.shape = tree_labels$node_shape, 
    vertex.label.color = tree_labels$vertex_label_color, vertex.size = 14, vertex.color = 'lightblue', edge.arrow.size = 0.3, 
    edge.label = tree_labels$arrow_lable, edge.label.color = 'black', edge.label.cex = 0.6, main = labels$tree_model, vertex.label.cex = 0.55)

```

