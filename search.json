[{"path":"https://markusul.github.io/SDForest/articles/Algorithm.html","id":"sdtree-subroutine","dir":"Articles","previous_headings":"","what":"SDTree subroutine","title":"Algorithm","text":"lines 4-7 algorithm, one seeks find split (b,j,s)(b, j, s) among candidate splits lead largest decrease Œ±b,j,s\\alpha_{b, j, s} spectral objective ‚à•Q(ùêò‚àíùí´cÃÇ)‚à•\\|Q(\\mathbf Y - \\mathcal P \\hat c)\\|. Naively, every candidate split, one update indicator matrix ùí´\\mathcal P, calculate corresponding least squares estimator cÃÇ\\hat c plug obtain loss decrease. Using following procedure, decrease loss can calculated efficiently. initialize, set u1‚Ä≤=Q‚ãÖ(1,‚Ä¶,1)Tu_1' = Q\\cdot (1,\\ldots, 1)^T u1=u1‚Ä≤/‚à•u1‚Ä≤‚à•u_1 = u_1'/\\|u_1'\\|. Inductively, assume now step MM loop line 3 algorithm defined vectors u1,‚Ä¶,uM‚àà‚Ñùnu_1, \\ldots, u_M\\\\mathbb R^n. step, evaluate new potential splits parameterized (b,j,s)(b, j, s) (see lines 4-6 algorithm). new splits can encoded vector e=e(b,j,s)‚àà{0,1}ne = e^{(b, j,s)}\\\\{0,1\\}^n set {j‚àà{1,‚Ä¶,n}:ej=1}\\{j\\\\{1,\\ldots, n\\}: e_j = 1\\} subset set {j‚àà{1,‚Ä¶,n}:ùí´ÃÇj,m}\\{j\\\\{1,\\ldots, n\\}: \\hat {\\mathcal P}_{j, m}\\} m‚àà{1,‚Ä¶,M}m\\\\{1, \\ldots, M\\}. candidate split encoded ee, define u‚Ä≤(e)=(Q‚àí‚àël=1MululTQ)eu'(e) = (Q-\\sum_{l=1}^M u_l u_l^T Q)e Œ±(e)=(u‚Ä≤(e)TQY)2/‚à•u‚Ä≤(e)‚à•22.\\alpha(e) = (u'(e)^T QY)^2/\\|u'(e)\\|_2^2. , Œ±b,j,s\\alpha_{b, j, s} according line 6 algorithm defined Œ±b,j,s:=Œ±(e(b,j,s))\\alpha_{b, j, s} := \\alpha(e^{(b, j, s)}). Finally, define uM+1=u‚Ä≤(e(b*,j*,s*))/‚à•u‚Ä≤(e(b*,j*,s*))‚à•2u_{M+1} = u'(e^{(b^*, j^*, s^*)})/\\|u'(e^{(b^*, j^*, s^*)})\\|_2 (b*,j*,s*)(b^*, j^*, s^*) defined line 9 algorithm.","code":""},{"path":"https://markusul.github.io/SDForest/articles/Algorithm.html","id":"sdforest","dir":"Articles","previous_headings":"","what":"SDForest","title":"Algorithm","text":"spectral deconfounded Random Forest combines SDTrees way, original Random Forest (Breiman 2001). idea combine multiple regression trees ensemble order decrease variance get smooth function. Ensembles work best different models independent . decorrelate regression trees much possible , two mechanisms. first one bagging (Breiman 1996), train regression tree independent bootstrap sample observations, e.g., draw random sample size nn replacement observations. second mechanic decrease correlation random subset covariates available split. split, sample mtry‚â§p\\text{mtry} \\leq p covariates choose one reduces loss . f(X)ÃÇ=1Ntree‚àët=1NtreeSDTreetboot(X)\\widehat{f(X)} = \\frac{1}{N_{tree}} \\sum_{t = 1}^{N_{tree}} SDTree_t^{boot}(X)","code":""},{"path":"https://markusul.github.io/SDForest/articles/Runtime.html","id":"computations","dir":"Articles","previous_headings":"","what":"Computations","title":"Runtime","text":"speedup can achieved taking advantage modern hardware.","code":""},{"path":"https://markusul.github.io/SDForest/articles/Runtime.html","id":"multicore","dir":"Articles","previous_headings":"Computations","what":"Multicore","title":"Runtime","text":"estimating SDForest, obvious way increase computations fit individual trees different cores parallel. Parallel computing supported Unix Windows. Depending system set , linear algebra libraries might already run parallel. case, speed improvement choosing one core run might large.","code":"# fits the individual SDTrees in parallel on 22 cores fit <- SDForest(x = X, y = Y, mc.cores = 22)"},{"path":"https://markusul.github.io/SDForest/articles/Runtime.html","id":"gpu","dir":"Articles","previous_headings":"Computations","what":"GPU","title":"Runtime","text":"Especially many observations, might reasonable perform matrix multiplications GPU. can evaluate many potential splits simultaneously multiplying n times n matrix n times potential split matrix GPU. use GPUmatrix (Lobato-Fernandez, .Ferrer-Bonsoms, Rubio 2024) calculations GPU. also refer website set GPU properly. number splits can evaluated parallel way highly depends GPU size can controlled using mem_size parameter. default value 1e+7 result memory overflow GPU 24G VRAM. us, worked well GeForce RTX 3090.","code":"# runs the matrix operations on a gpu if available fit <- SDForest(x = X, y = Y, gpu = T, mem_size = 1e+7) tree <- SDTree(x = X, y = Y, gpu = T, mem_size = 1e+7)"},{"path":"https://markusul.github.io/SDForest/articles/Runtime.html","id":"approximations","dir":"Articles","previous_headings":"","what":"Approximations","title":"Runtime","text":"places, approximations perform almost well run whole procedure. Reasonable split points divide space ‚Ñùp\\mathbb{R}^p , principle, values observed ones. practice many observations, number potential splits grows large. , therefore, evaluate maximal max_candidates splits potential ones choose according quantiles potential ones. many observations, can reduce computing time sampling max_size observations data instead nn. can dramatically reduce computing time compared full bootstrap sample also decrease performance.","code":"# approximation of candidate splits fit <- SDForest(x = X, y = Y, max_candidates = 100) tree <- SDTree(x = X, y = Y, max_candidates = 50) # draws maximal 500 samples from the data for each tree fit <- SDForest(x = X, y = Y, max_size = 500)"},{"path":"https://markusul.github.io/SDForest/articles/SDForest.html","id":"causal-parents","dir":"Articles","previous_headings":"","what":"Causal parents","title":"SDForest","text":"Given estimated causal function, first question might want answer covariates causal parents response. intervene causal parents, expect response change. , can estimate functional dependency YY XX, f(X)ÃÇ\\widehat{f(X)} examine covariates important function. can directly compare importance pattern deconfounded estimator classical random forest estimated ranger. comparison plain counterpart always gives feeling strength confounding. confounding exists, SDForest() ranger::ranger() result similar models. graph , see variable importance varImp() deconfounded random forest plain random forest. scale meaning, see true causal parents red getting clear higher variable importance SDForest. plain random forest distinguish spurious correlation true causation.  , looked variable importance non-regularized SDForest. two techniques better understand variables might causally important. first regularization path regPath(), plot variable importance varying regularization, .e.¬†different values. option lets us visualize paths interactively better understand covariates seem robust importance model.  second method follows stability selection approach (Meinshausen B√ºhlmann 2010). , visualize proportion trees forest use covariate splits model. regularize , truly causal important variables still used trees.","code":"# comparison to classical random forest fit_ranger <- ranger::ranger(Y ~ ., train_data, importance = 'impurity')  # comparison of variable importance imp_ranger <- fit_ranger$variable.importance imp_sdf <- fit$var_importance imp_col <- rep('black', length(imp_ranger)) imp_col[sim_data$j] <- 'red'  plot(imp_ranger, imp_sdf, col = imp_col, pch = 20,      xlab = 'ranger', ylab = 'SDForest',       main = 'Variable Importance') # check regularization path of variable importance path <- regPath(fit) plot(path) # select 20 most important covariates for further exploration most_imp <- fit$var_importance > sort(fit$var_importance, decreasing = TRUE)[20] plot(path, plotly = TRUE, most_imp) # detection of causal parent using stability selection stablePath <- stabilitySelection(fit) #plot(stablePath, plotly = TRUE) plot(stablePath)"},{"path":"https://markusul.github.io/SDForest/articles/SDForest.html","id":"causal-dependence","dir":"Articles","previous_headings":"","what":"Causal dependence","title":"SDForest","text":"finding causal parent response, one might interested partial functional dependence YY causal parents. want intervene system, need know intervene also intervene order get desired response. , first prune forest remove residual spurious correlation get optimal predictive power. , regPath() also contains --bag prediction errors different regularizations. plotOOB() visualizes mean squared error (oob.MSE) spectral loss (oob.SDE) minimize. minimal --bag error lets us choose optimal value prune forest.  Now, distil partial functional dependence response causal parents, use partial dependence plots partDependence() (Friedman 2001). , vary value one covariate fixing others ones actually observe data. representative partial conditional dependence, plot mean individual response curves addition sample individuals.","code":"# out of bag error for different regularization plotOOB(path) # pruning of forest according to optimal out-of-bag performance fit <- prune(fit, cp = path$cp_min) # partial functional dependence of y on the first causal parent dep <- partDependence(fit, sim_data$j) plot(dep)"},{"path":"https://markusul.github.io/SDForest/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Markus Ulmer. Author, maintainer. Cyrill Scheidegger. Author.","code":""},{"path":"https://markusul.github.io/SDForest/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Ulmer M, Scheidegger C (2024). SDForest: Spectrally Deconfounded Random Forest. R package version 0.1, https://markusul.github.io/SDForest/.","code":"@Manual{,   title = {SDForest: Spectrally Deconfounded Random Forest},   author = {Markus Ulmer and Cyrill Scheidegger},   year = {2024},   note = {R package version 0.1},   url = {https://markusul.github.io/SDForest/}, }"},{"path":"https://markusul.github.io/SDForest/index.html","id":"sdforest","dir":"","previous_headings":"","what":"Spectrally Deconfounded Random Forest","title":"Spectrally Deconfounded Random Forest","text":"Spectral Deconfounded Random Forests (SDForest) method estimating non-linear sparse causal effects presence unobserved confounding. SDForest shown nice estimate true causal function settings, observe many covariates, e.g.¬†high-dimensional setting, fairly sparse confounding. (Guo, ƒÜevid, B√ºhlmann (2022), ƒÜevid, B√ºhlmann, Meinshausen (2020))","code":""},{"path":"https://markusul.github.io/SDForest/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Spectrally Deconfounded Random Forest","text":"can install development version SDForest GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"markusul/SDForest\")  # or # install.packages('pak') # pak::pkg_install('markusul/SDForest')"},{"path":"https://markusul.github.io/SDForest/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Spectrally Deconfounded Random Forest","text":"basic example estimate causal effect XX YY using SDForest. can learn analyzing sparse causal effects estimated SDForest vignette(\"SDForest\"). can also estimate just one Spectral Deconfounded Regression Tree using SDTree function. See also vignette(\"SDTree\").","code":"library(SDForest)  set.seed(42) # simulation of confounded data sim_data <- simulate_data_nonlinear(q = 2, p = 50, n = 100, m = 2) X <- sim_data$X Y <- sim_data$Y train_data <- data.frame(X, Y) # causal parents sim_data$j #> [1] 16 40 fit <- SDForest(Y ~ ., train_data) fit #> SDForest result #>  #> Number of trees:  100  #> Number of covariates:  50  #> OOB loss:  0.06  #> OOB spectral loss:  0.03 causal_Tree <- SDTree(Y ~ ., train_data, cp = 0.03)  # plot the causal tree causal_Tree #>   levelName     value          s  j       label decision n_samples #> 1 1         0.8398920  0.5322484 40 X40 <= 0.53                100 #> 2  ¬¶--1     0.6672222 -0.4974688 40 X40 <= -0.5       no        73 #> 3  ¬¶   ¬¶--1 0.5876494         NA NA         0.6       no        44 #> 4  ¬¶   ¬∞--4 0.7491725         NA NA         0.7      yes        29 #> 5  ¬∞--2     1.1482966  2.4494222 40 X40 <= 2.45      yes        27 #> 6      ¬¶--2 1.0937407         NA NA         1.1       no        21 #> 7      ¬∞--3 1.6619968         NA NA         1.7      yes         6 plot(causal_Tree)"},{"path":"https://markusul.github.io/SDForest/reference/SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Spectrally Deconfounded Random Forests ‚Äî SDForest","title":"Spectrally Deconfounded Random Forests ‚Äî SDForest","text":"Estimate regression Random Forest using spectral deconfounding. spectrally deconfounded Random Forest (SDForest) combines SDTrees way, original Random Forest (Breiman 2001) . idea combine multiple regression trees ensemble order decrease variance get smooth function. Ensembles work best different models independent . decorrelate regression trees much possible , two mechanisms. first one bagging (Breiman 1996) , train regression tree independent bootstrap sample observations, e.g., draw random sample size \\(n\\) replacement observations. second mechanic decrease correlation random subset covariates available split. split, sample \\(\\text{mtry} \\leq p\\) covariates choose one reduces loss . $$\\widehat{f(X)} = \\frac{1}{N_{tree}} \\sum_{t = 1}^{N_{tree}} SDTree_t(X)$$","code":""},{"path":"https://markusul.github.io/SDForest/reference/SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spectrally Deconfounded Random Forests ‚Äî SDForest","text":"","code":"SDForest(   formula = NULL,   data = NULL,   x = NULL,   y = NULL,   nTree = 100,   cp = 0,   min_sample = 5,   mtry = NULL,   mc.cores = 1,   Q_type = \"trim\",   trim_quantile = 0.5,   q_hat = 0,   Q = NULL,   A = NULL,   gamma = 7,   max_size = NULL,   gpu = FALSE,   return_data = TRUE,   mem_size = 1e+07,   leave_out_ind = NULL,   envs = NULL,   nTree_leave_out = NULL,   nTree_env = NULL,   max_candidates = 100 )"},{"path":"https://markusul.github.io/SDForest/reference/SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spectrally Deconfounded Random Forests ‚Äî SDForest","text":"formula Object class formula describing model fit form y ~ x1 + x2 + ... y numeric response x1, x2, ... vectors covariates. Interactions supported. data Training data class data.frame containing variables model. x Matrix covariates, alternative formula data. y Vector responses, alternative formula data. nTree Number trees grow. cp Complexity parameter, minimum loss decrease split node. split performed loss decrease larger cp * initial_loss, initial_loss loss initial estimate using stump. min_sample Minimum number observations per leaf. split performed resulting leaves least min_sample observations. mtry Number randomly selected covariates consider split, NULL half covariates available split. \\(\\text{mtry} = \\lfloor \\frac{p}{2} \\rfloor\\) mc.cores Number cores use parallel processing, mc.cores > 1 trees estimated parallel. Q_type Type deconfounding, one 'trim', 'pca', 'no_deconfounding'. 'trim' corresponds Trim transform (ƒÜevid et al. 2020)  implemented Doubly debiased lasso (Guo et al. 2022) , 'pca' PCA transformation(Paul et al. 2008) . See get_Q. trim_quantile Quantile Trim transform, needed trim, see get_Q. q_hat Assumed confounding dimension, needed pca, see get_Q. Q Spectral transformation, NULL internally estimated using get_Q. Numerical Anchor class matrix. See get_W. gamma Strength distributional robustness, \\(\\gamma \\[0, \\infty]\\). See get_W. max_size Maximum number observations used bootstrap sample. NULL n samples replacement drawn. gpu TRUE, calculations performed GPU. properly set . return_data TRUE, training data returned output. needed prune.SDForest, regPath.SDForest, mergeForest. mem_size Amount split candidates can evaluated . trade-memory speed can decreased either memory sufficient gpu small. leave_out_ind Indices observations used training. envs Vector environments class factor can used stratified tree fitting. nTree_leave_out Number trees estimated leaving one environments . Results number environments times number trees. nTree_env Number trees estimated environment. Results number environments times number trees. max_candidates Maximum number split points proposed node covariate.","code":""},{"path":"https://markusul.github.io/SDForest/reference/SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Spectrally Deconfounded Random Forests ‚Äî SDForest","text":"Object class SDForest containing: predictions Vector predictions observation. forest List SDTree objects. var_names Names covariates. oob_loss --bag loss. MSE oob_SDloss --bag loss using spectral transformation. var_importance Variable importance. variable importance calculated sum decrease loss function resulting splits use covariate tree. mean variable importance trees results variable importance forest. oob_ind List indices trees contain observation training set. oob_predictions --bag predictions. return_data TRUE following also returned: X Matrix covariates. Y Vector responses. Q Spectral transformation matrix. envs provided following also returned: envs Vector environments. nTree_env Number trees environment. ooEnv_ind List indices trees contain observation environment training set observation. ooEnv_loss --bag loss using trees contain observation environment. ooEnv_SDloss --bag loss using spectral transformation trees contain observation environment. ooEnv_predictions --bag predictions using trees contain observation environment. nTree_leave_out environments left , environment tree, left . nTree_env environments provided, environment tree trained .","code":""},{"path":"https://markusul.github.io/SDForest/reference/SDForest.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Spectrally Deconfounded Random Forests ‚Äî SDForest","text":"Breiman L (1996). ‚ÄúBagging predictors.‚Äù Machine Learning, 24(2), 123‚Äì140. ISSN 0885-6125, doi:10.1007/BF00058655 . Breiman L (2001). ‚ÄúRandom Forests.‚Äù Machine Learning, 45(1), 5‚Äì32. ISSN 08856125, doi:10.1023/:1010933404324 . ƒÜevid D, B√ºhlmann P, Meinshausen N (2020). ‚ÄúSpectral Deconfounding via Perturbed Sparse Linear Models.‚Äù J. Mach. Learn. Res., 21(1). ISSN 1532-4435. Guo Z, ƒÜevid D, B√ºhlmann P (2022). ‚ÄúDoubly debiased lasso: High-dimensional inference hidden confounding.‚Äù Annals Statistics, 50(3). ISSN 0090-5364, doi:10.1214/21-AOS2152 . Paul D, Bair E, Hastie T, Tibshirani R (2008). ‚Äú‚ÄúPreconditioning‚Äù feature selection regression high-dimensional problems.‚Äù Annals Statistics, 36(4). ISSN 0090-5364, doi:10.1214/009053607000000578 .","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Spectrally Deconfounded Random Forests ‚Äî SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/SDForest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Spectrally Deconfounded Random Forests ‚Äî SDForest","text":"","code":"set.seed(42) # simulation of confounded data sim_data <- simulate_data_nonlinear(q = 2, p = 150, n = 100, m = 2) X <- sim_data$X Y <- sim_data$Y train_data <- data.frame(X, Y) # causal parents of y sim_data$j #> [1]  88 112  # comparison to classical random forest fit_ranger <- ranger::ranger(Y ~ ., train_data, importance = 'impurity')  fit <- SDForest(x = X, y = Y, nTree = 10, Q_type = 'pca', q_hat = 2) fit <- SDForest(Y ~ ., train_data) fit #> SDForest result #>  #> Number of trees:  100  #> Number of covariates:  150  #> OOB loss:  0.66  #> OOB spectral loss:  0.07   # comparison of variable importance imp_ranger <- fit_ranger$variable.importance imp_sdf <- fit$var_importance imp_col <- rep('black', length(imp_ranger)) imp_col[sim_data$j] <- 'red'  plot(imp_ranger, imp_sdf, col = imp_col, pch = 20,      xlab = 'ranger', ylab = 'SDForest',       main = 'Variable Importance')   # check regularization path of variable importance path <- regPath(fit) # out of bag error for different regularization plotOOB(path)  plot(path)   # detection of causal parent using stability selection stablePath <- stabilitySelection(fit) plot(stablePath)   # pruning of forest according to optimal out-of-bag performance fit <- prune(fit, cp = path$cp_min)  # partial functional dependence of y on the most important covariate most_imp <- which.max(fit$var_importance) dep <- partDependence(fit, most_imp) plot(dep, n_examples = 100)"},{"path":"https://markusul.github.io/SDForest/reference/SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Spectrally Deconfounded Tree ‚Äî SDTree","title":"Spectrally Deconfounded Tree ‚Äî SDTree","text":"Estimates regression tree using spectral deconfounding. regression tree part function class step functions \\(f(X) = \\sum_{m = 1}^M 1_{\\{X \\R_m\\}} c_m\\), (\\(R_m\\)) \\(m = 1, \\ldots, M\\) regions dividing space \\(\\mathbb{R}^p\\) \\(M\\) rectangular parts. region response level \\(c_m \\\\mathbb{R}\\). training data, can write step function \\(f(\\mathbf{X}) = \\mathcal{P} c\\) \\(\\mathcal{P} \\\\{0, 1\\}^{n \\times M}\\) indicator matrix encoding region observation belongs \\(c \\\\mathbb{R}^M\\) vector containing levels corresponding different regions. function minimizes $$(\\hat{\\mathcal{P}}, \\hat{c}) = \\text{argmin}_{\\mathcal{P}' \\\\{0, 1\\}^{n \\times M}, c' \\\\mathbb{R}^ {M}} \\frac{||Q(\\mathbf{Y} - \\mathcal{P'} c')||_2^2}{n}$$ find \\(\\hat{\\mathcal{P}}\\) using tree structure repeated splitting leaves, similar original cart algorithm (Breiman et al. 2017) . Since comparing possibilities \\(\\mathcal{P}\\) impossible, let tree grow greedily. Given current tree, iterate leaves possible splits. choose one reduces spectral loss estimate split leave estimates \\(\\hat{c} = \\text{argmin}_{c' \\\\mathbb{R}^M} \\frac{||Q\\mathbf{Y} - Q\\mathcal{P} c'||_2^2}{n}\\) just linear regression problem. repeated loss decreases less minimum loss decrease split. minimum loss decrease equals cost-complexity parameter \\(cp\\) times initial loss overall mean estimated. cost-complexity parameter \\(cp\\) controls complexity regression tree acts regularization parameter.","code":""},{"path":"https://markusul.github.io/SDForest/reference/SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spectrally Deconfounded Tree ‚Äî SDTree","text":"","code":"SDTree(   formula = NULL,   data = NULL,   x = NULL,   y = NULL,   max_leaves = NULL,   cp = 0.01,   min_sample = 5,   mtry = NULL,   fast = TRUE,   Q_type = \"trim\",   trim_quantile = 0.5,   q_hat = 0,   Q = NULL,   A = NULL,   gamma = 0.5,   gpu = FALSE,   mem_size = 1e+07,   max_candidates = 100 )"},{"path":"https://markusul.github.io/SDForest/reference/SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spectrally Deconfounded Tree ‚Äî SDTree","text":"formula Object class formula describing model fit form y ~ x1 + x2 + ... y numeric response x1, x2, ... vectors covariates. Interactions supported. data Training data class data.frame containing variables model. x Matrix covariates, alternative formula data. y Vector responses, alternative formula data. max_leaves Maximum number leaves grown tree. cp Complexity parameter, minimum loss decrease split node. split performed loss decrease larger cp * initial_loss, initial_loss loss initial estimate using stump. min_sample Minimum number observations per leaf. split performed resulting leaves least min_sample observations. mtry Number randomly selected covariates consider split, NULL covariates available split. fast TRUE, optimal splits new leaves evaluated previously optimal splits potential loss-decrease reused. FALSE possible splits leaves reevaluated every split. Q_type Type deconfounding, one 'trim', 'pca', 'no_deconfounding'. 'trim' corresponds Trim transform (ƒÜevid et al. 2020)  implemented Doubly debiased lasso (Guo et al. 2022) , 'pca' PCA transformation(Paul et al. 2008) . See get_Q. trim_quantile Quantile Trim transform, needed trim, see get_Q. q_hat Assumed confounding dimension, needed pca, see get_Q. Q Spectral transformation, NULL internally estimated using get_Q. Numerical Anchor class matrix. See get_W. gamma Strength distributional robustness, \\(\\gamma \\[0, \\infty]\\). See get_W. gpu TRUE, calculations performed GPU. properly set . mem_size Amount split candidates can evaluated . trade-memory speed can decreased either memory sufficient gpu small. max_candidates Maximum number split points proposed node covariate.","code":""},{"path":"https://markusul.github.io/SDForest/reference/SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Spectrally Deconfounded Tree ‚Äî SDTree","text":"Object class SDTree containing predictions Predictions training set. tree estimated tree class Node (Glur 2023) . tree contains information splits resulting estimates. var_names Names covariates training data. var_importance Variable importance covariates. variable importance calculated sum decrease loss function resulting splits use covariate.","code":""},{"path":"https://markusul.github.io/SDForest/reference/SDTree.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Spectrally Deconfounded Tree ‚Äî SDTree","text":"Breiman L, Friedman JH, Olshen RA, Stone CJ (2017). Classification Regression Trees. Routledge. ISBN 9781315139470, doi:10.1201/9781315139470 . ƒÜevid D, B√ºhlmann P, Meinshausen N (2020). ‚ÄúSpectral Deconfounding via Perturbed Sparse Linear Models.‚Äù J. Mach. Learn. Res., 21(1). ISSN 1532-4435. Glur C (2023). ‚Äúdata.tree: General Purpose Hierarchical Data Structure.‚Äù https://CRAN.R-project.org/package=data.tree. Guo Z, ƒÜevid D, B√ºhlmann P (2022). ‚ÄúDoubly debiased lasso: High-dimensional inference hidden confounding.‚Äù Annals Statistics, 50(3). ISSN 0090-5364, doi:10.1214/21-AOS2152 . Paul D, Bair E, Hastie T, Tibshirani R (2008). ‚Äú‚ÄúPreconditioning‚Äù feature selection regression high-dimensional problems.‚Äù Annals Statistics, 36(4). ISSN 0090-5364, doi:10.1214/009053607000000578 .","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Spectrally Deconfounded Tree ‚Äî SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/SDTree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Spectrally Deconfounded Tree ‚Äî SDTree","text":"","code":"set.seed(42) # simulation of confounded data sim_data <- simulate_data_step(q = 2, p = 150, n = 100, m = 2) X <- sim_data$X Y <- sim_data$Y train_data <- data.frame(X, Y) # causal parents of y sim_data$j #> [1] 26 65  tree_plain_cv <- cvSDTree(Y ~ ., train_data, Q_type = \"no_deconfounding\") tree_plain <- SDTree(Y ~ ., train_data, Q_type = \"no_deconfounding\", cp = 0)  tree_causal_cv <- cvSDTree(Y ~ ., train_data) tree_causal <- SDTree(y = Y, x = X, cp = 0)  # check regularization path of variable importance path <- regPath(tree_causal) plot(path)   tree_plain <- prune(tree_plain, cp = tree_plain_cv$cp_min) tree_causal <- prune(tree_causal, cp = tree_causal_cv$cp_min) plot(tree_causal)  {\"x\":{\"diagram\":\"digraph {\\n\\n\\n\\n\\n  \\\"1\\\" [label = \\\"X65 <= -2.02\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"2\\\" [label = \\\"-23.2\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"3\\\" [label = \\\"X95 <= -0.67\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"4\\\" [label = \\\"X28 <= -1.57\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"5\\\" [label = \\\"18.4\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"6\\\" [label = \\\"16.8\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"7\\\" [label = \\\"20.2\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n\\\"1\\\"->\\\"2\\\" [label = \\\"no\\\"] \\n\\\"1\\\"->\\\"3\\\" [label = \\\"yes\\\"] \\n\\\"3\\\"->\\\"4\\\" [label = \\\"no\\\"] \\n\\\"3\\\"->\\\"7\\\" [label = \\\"yes\\\"] \\n\\\"4\\\"->\\\"5\\\" [label = \\\"no\\\"] \\n\\\"4\\\"->\\\"6\\\" [label = \\\"yes\\\"] \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}  plot(tree_plain)  {\"x\":{\"diagram\":\"digraph {\\n\\n\\n\\n\\n  \\\"1\\\" [label = \\\"X40 <= -0.24\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"2\\\" [label = \\\"X142 <= 2.89\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"3\\\" [label = \\\"10.2\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"4\\\" [label = \\\"-10.4\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"5\\\" [label = \\\"X89 <= -1.52\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"6\\\" [label = \\\"39.3\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"7\\\" [label = \\\"21.3\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n\\\"1\\\"->\\\"2\\\" [label = \\\"no\\\"] \\n\\\"1\\\"->\\\"5\\\" [label = \\\"yes\\\"] \\n\\\"2\\\"->\\\"3\\\" [label = \\\"no\\\"] \\n\\\"2\\\"->\\\"4\\\" [label = \\\"yes\\\"] \\n\\\"5\\\"->\\\"6\\\" [label = \\\"no\\\"] \\n\\\"5\\\"->\\\"7\\\" [label = \\\"yes\\\"] \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}"},{"path":"https://markusul.github.io/SDForest/reference/copy.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Copy a forest ‚Äî copy.SDForest","title":"Copy a forest ‚Äî copy.SDForest","text":"Returns copy forest object. Might useful want keep original forest comparison pruned forest.","code":""},{"path":"https://markusul.github.io/SDForest/reference/copy.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Copy a forest ‚Äî copy.SDForest","text":"","code":"# S3 method for class 'SDForest' copy(object, ...)"},{"path":"https://markusul.github.io/SDForest/reference/copy.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Copy a forest ‚Äî copy.SDForest","text":"object SDForest object ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/copy.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Copy a forest ‚Äî copy.SDForest","text":"copy SDForest object","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/copy.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Copy a forest ‚Äî copy.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/copy.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Copy a tree ‚Äî copy.SDTree","title":"Copy a tree ‚Äî copy.SDTree","text":"Returns copy tree object. Might useful want keep original tree comparison pruned tree.","code":""},{"path":"https://markusul.github.io/SDForest/reference/copy.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Copy a tree ‚Äî copy.SDTree","text":"","code":"# S3 method for class 'SDTree' copy(object, ...)"},{"path":"https://markusul.github.io/SDForest/reference/copy.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Copy a tree ‚Äî copy.SDTree","text":"object SDTree object ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/copy.SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Copy a tree ‚Äî copy.SDTree","text":"copy SDTree object","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/copy.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Copy a tree ‚Äî copy.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/cvSDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for the SDTree ‚Äî cvSDTree","title":"Cross-validation for the SDTree ‚Äî cvSDTree","text":"Estimates optimal complexity parameter SDTree using cross-validation. transformations estimated training set validation set separately ensure independence validation set.","code":""},{"path":"https://markusul.github.io/SDForest/reference/cvSDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for the SDTree ‚Äî cvSDTree","text":"","code":"cvSDTree(   formula = NULL,   data = NULL,   x = NULL,   y = NULL,   max_leaves = NULL,   cp = 0.01,   min_sample = 5,   mtry = NULL,   fast = TRUE,   Q_type = \"trim\",   trim_quantile = 0.5,   q_hat = 0,   Q = NULL,   A = NULL,   gamma = 0.5,   gpu = FALSE,   mem_size = 1e+07,   max_candidates = 100,   n_cv = 3,   cp_seq = NULL,   mc.cores = 1 )"},{"path":"https://markusul.github.io/SDForest/reference/cvSDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for the SDTree ‚Äî cvSDTree","text":"formula Object class formula describing model fit form y ~ x1 + x2 + ... y numeric response x1, x2, ... vectors covariates. Interactions supported. data Training data class data.frame containing variables model. x Predictor data, alternative formula data. y Response vector, alternative formula data. max_leaves Maximum number leaves grown tree. cp Complexity parameter, minimum loss decrease split node. split performed loss decrease larger cp * initial_loss, initial_loss loss initial estimate using stump. min_sample Minimum number observations per leaf. split performed resulting leaves least min_sample observations. mtry Number randomly selected covariates consider split, NULL covariates available split. fast TRUE, optimal splits new leaves evaluated previously optimal splits potential loss-decrease reused. FALSE possible splits leaves reevaluated every split. Q_type Type deconfounding, one 'trim', 'pca', 'no_deconfounding'. 'trim' corresponds Trim transform (ƒÜevid et al. 2020)  implemented Doubly debiased lasso (Guo et al. 2022) , 'pca' PCA transformation(Paul et al. 2008) . See get_Q. trim_quantile Quantile Trim transform, needed trim DDL_trim, see get_Q. q_hat Assumed confounding dimension, needed pca, see get_Q. Q Spectral transformation, NULL internally estimated using get_Q. Numerical Anchor class matrix. See get_W. gamma Strength distributional robustness, \\(\\gamma \\[0, \\infty]\\). See get_W. gpu TRUE, calculations performed GPU. properly set . mem_size Amount split candidates can evaluated . trade-memory speed can decreased either memory sufficient gpu small. max_candidates Maximum number split points proposed node covariate. n_cv Number folds cross-validation. recommended use 5 folds number covariates larger number observations. case spectral transformation differ much validation data substantially smaller training data. cp_seq Sequence complexity parameters cp compare using cross-validation, NULL sequence 0 0.6 stepsize 0.002 used. mc.cores Number cores use parallel computation.","code":""},{"path":"https://markusul.github.io/SDForest/reference/cvSDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for the SDTree ‚Äî cvSDTree","text":"list containing cp_min optimal complexity parameter. cp_table table containing complexity parameter, mean standard deviation loss validation sets complexity parameters. multiple complexity parameters result loss, one largest complexity parameter shown.","code":""},{"path":"https://markusul.github.io/SDForest/reference/cvSDTree.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cross-validation for the SDTree ‚Äî cvSDTree","text":"ƒÜevid D, B√ºhlmann P, Meinshausen N (2020). ‚ÄúSpectral Deconfounding via Perturbed Sparse Linear Models.‚Äù J. Mach. Learn. Res., 21(1). ISSN 1532-4435. Guo Z, ƒÜevid D, B√ºhlmann P (2022). ‚ÄúDoubly debiased lasso: High-dimensional inference hidden confounding.‚Äù Annals Statistics, 50(3). ISSN 0090-5364, doi:10.1214/21-AOS2152 . Paul D, Bair E, Hastie T, Tibshirani R (2008). ‚Äú‚ÄúPreconditioning‚Äù feature selection regression high-dimensional problems.‚Äù Annals Statistics, 36(4). ISSN 0090-5364, doi:10.1214/009053607000000578 .","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/cvSDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cross-validation for the SDTree ‚Äî cvSDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/cvSDTree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for the SDTree ‚Äî cvSDTree","text":"","code":"set.seed(1) n <- 50 X <- matrix(rnorm(n * 5), nrow = n) y <- sign(X[, 1]) * 3 + rnorm(n, 0, 5) cp <- cvSDTree(x = X, y = y, Q_type = 'no_deconfounding') cp #> $cp_min #>    cp  #> 0.076  #>  #> $cp_table #>          cp SDLoss mean SDLoss sd #>  [1,] 0.044    30.89977  4.336556 #>  [2,] 0.056    30.65170  5.398475 #>  [3,] 0.058    30.60373  5.370633 #>  [4,] 0.070    31.28713  5.866639 #>  [5,] 0.076    29.12192  5.155334 #>  [6,] 0.106    31.64469  3.284572 #>  [7,] 0.132    32.25506  3.564472 #>  [8,] 0.262    30.93818  5.837083 #>  [9,] 0.326    31.50641  6.333261 #> [10,] 0.362    33.27446  3.286796 #> [11,] 0.600    34.64608  4.718930 #>"},{"path":"https://markusul.github.io/SDForest/reference/f_four.html","id":null,"dir":"Reference","previous_headings":"","what":"Function of x on a fourier basis ‚Äî f_four","title":"Function of x on a fourier basis ‚Äî f_four","text":"Function x fourier basis subset covariates causal effect Y using parameters beta. function given : $$f(x_i) = \\sum_{j = 1}^p 1_{j \\js} \\sum_{k = 1}^K (\\beta_{j, k}^{(1)} \\cos(0.2 k x_j) + \\beta_{j, k}^{(2)} \\sin(0.2 k x_j))$$","code":""},{"path":"https://markusul.github.io/SDForest/reference/f_four.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function of x on a fourier basis ‚Äî f_four","text":"","code":"f_four(x, beta, js)"},{"path":"https://markusul.github.io/SDForest/reference/f_four.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function of x on a fourier basis ‚Äî f_four","text":"x vector covariates beta parameter vector function f(X) js indices causal covariates X","code":""},{"path":"https://markusul.github.io/SDForest/reference/f_four.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function of x on a fourier basis ‚Äî f_four","text":"value function f(x)","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/f_four.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Function of x on a fourier basis ‚Äî f_four","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"SDForest fromList method ‚Äî fromList.SDForest","title":"SDForest fromList method ‚Äî fromList.SDForest","text":"Converts trees SDForest object class list class Node (Glur 2023) .","code":""},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SDForest fromList method ‚Äî fromList.SDForest","text":"","code":"# S3 method for class 'SDForest' fromList(object, ...)"},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SDForest fromList method ‚Äî fromList.SDForest","text":"object SDForest object trees list format ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SDForest fromList method ‚Äî fromList.SDForest","text":"SDForest object trees Node format","code":""},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDForest.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SDForest fromList method ‚Äî fromList.SDForest","text":"Glur C (2023). ‚Äúdata.tree: General Purpose Hierarchical Data Structure.‚Äù https://CRAN.R-project.org/package=data.tree.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SDForest fromList method ‚Äî fromList.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"SDTree fromList method ‚Äî fromList.SDTree","title":"SDTree fromList method ‚Äî fromList.SDTree","text":"Converts tree SDTree object class list class Node (Glur 2023) .","code":""},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SDTree fromList method ‚Äî fromList.SDTree","text":"","code":"# S3 method for class 'SDTree' fromList(object, ...)"},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SDTree fromList method ‚Äî fromList.SDTree","text":"object SDTree object tree list format ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SDTree fromList method ‚Äî fromList.SDTree","text":"SDTree object tree Node format","code":""},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDTree.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SDTree fromList method ‚Äî fromList.SDTree","text":"Glur C (2023). ‚Äúdata.tree: General Purpose Hierarchical Data Structure.‚Äù https://CRAN.R-project.org/package=data.tree.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/fromList.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SDTree fromList method ‚Äî fromList.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_Q.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimation of spectral transformation ‚Äî get_Q","title":"Estimation of spectral transformation ‚Äî get_Q","text":"Estimates spectral transformation Q spectral deconfounding shrinking leading singular values covariates.","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_Q.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimation of spectral transformation ‚Äî get_Q","text":"","code":"get_Q(X, type, trim_quantile = 0.5, q_hat = 0, gpu = FALSE, scaling = TRUE)"},{"path":"https://markusul.github.io/SDForest/reference/get_Q.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimation of spectral transformation ‚Äî get_Q","text":"X Numerical covariates class matrix. type Type deconfounding, one 'trim', 'pca', 'no_deconfounding'. 'trim' corresponds Trim transform (ƒÜevid et al. 2020)  implemented Doubly debiased lasso (Guo et al. 2022) , 'pca' PCA transformation(Paul et al. 2008)  'no_deconfounding' Identity. trim_quantile Quantile Trim transform, needed trim. q_hat Assumed confounding dimension, needed pca. gpu TRUE, calculations performed GPU. properly set . scaling Whether X scaled calculating spectral transformation.","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_Q.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimation of spectral transformation ‚Äî get_Q","text":"Q class matrix, spectral transformation matrix.","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_Q.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimation of spectral transformation ‚Äî get_Q","text":"ƒÜevid D, B√ºhlmann P, Meinshausen N (2020). ‚ÄúSpectral Deconfounding via Perturbed Sparse Linear Models.‚Äù J. Mach. Learn. Res., 21(1). ISSN 1532-4435. Guo Z, ƒÜevid D, B√ºhlmann P (2022). ‚ÄúDoubly debiased lasso: High-dimensional inference hidden confounding.‚Äù Annals Statistics, 50(3). ISSN 0090-5364, doi:10.1214/21-AOS2152 . Paul D, Bair E, Hastie T, Tibshirani R (2008). ‚Äú‚ÄúPreconditioning‚Äù feature selection regression high-dimensional problems.‚Äù Annals Statistics, 36(4). ISSN 0090-5364, doi:10.1214/009053607000000578 .","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_Q.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Estimation of spectral transformation ‚Äî get_Q","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_Q.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimation of spectral transformation ‚Äî get_Q","text":"","code":"set.seed(1) X <- matrix(rnorm(50 * 20), nrow = 50) Q_trim <- get_Q(X, 'trim') Q_pca <- get_Q(X, 'pca', q_hat = 5) Q_plain <- get_Q(X, 'no_deconfounding')"},{"path":"https://markusul.github.io/SDForest/reference/get_W.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimation of anchor transformation ‚Äî get_W","title":"Estimation of anchor transformation ‚Äî get_W","text":"Estimates anchor transformation Anchor-Objective. anchor transformation \\(W = -(1-\\sqrt{\\gamma}))\\Pi_A\\), \\(\\Pi_A = (^TA)^{-1}^T\\). \\(\\gamma = 1\\) just identity. \\(\\gamma = 0\\) corresponds residuals orthogonal projecting onto . large \\(\\gamma\\) close orthogonal projection onto , scaled \\(\\gamma\\). estimator \\(\\text{argmin}_f ||W(Y - f(X))||^2\\) corresponds Anchor-Regression Estimator (Rothenh√§usler et al. 2021) , (B√ºhlmann 2020) .","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_W.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimation of anchor transformation ‚Äî get_W","text":"","code":"get_W(A, gamma, intercept = FALSE, gpu = FALSE)"},{"path":"https://markusul.github.io/SDForest/reference/get_W.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimation of anchor transformation ‚Äî get_W","text":"Numerical Anchor class matrix. gamma Strength distributional robustness, \\(\\gamma \\[0, \\infty]\\). intercept Logical, whether include intercept anchor. gpu TRUE, calculations performed GPU. properly set .","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_W.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimation of anchor transformation ‚Äî get_W","text":"W class matrix, anchor transformation matrix.","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_W.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Estimation of anchor transformation ‚Äî get_W","text":"B√ºhlmann P (2020). ‚ÄúInvariance, Causality Robustness.‚Äù Statistical Science, 35(3). ISSN 0883-4237, doi:10.1214/19-STS721 . Rothenh√§usler D, Meinshausen N, B√ºhlmann P, Peters J (2021). ‚ÄúAnchor Regression: Heterogeneous Data Meet Causality.‚Äù Journal Royal Statistical Society Series B: Statistical Methodology, 83(2), 215‚Äì246. ISSN 1369-7412, doi:10.1111/rssb.12398 .","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_W.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Estimation of anchor transformation ‚Äî get_W","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_W.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimation of anchor transformation ‚Äî get_W","text":"","code":"set.seed(1) n <- 50 X <- matrix(rnorm(n * 1), nrow = n) Y <- 3 * X + rnorm(n) W <- get_W(X, gamma = 0) resid <- W %*% Y"},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the sequence of complexity parameters of an SDForest ‚Äî get_cp_seq.SDForest","title":"Get the sequence of complexity parameters of an SDForest ‚Äî get_cp_seq.SDForest","text":"function extracts sequence complexity parameters SDForest result changes SDForest pruned. cp values differ first three digits decimal point returned.","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the sequence of complexity parameters of an SDForest ‚Äî get_cp_seq.SDForest","text":"","code":"# S3 method for class 'SDForest' get_cp_seq(object, ...)"},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the sequence of complexity parameters of an SDForest ‚Äî get_cp_seq.SDForest","text":"object SDForest object ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the sequence of complexity parameters of an SDForest ‚Äî get_cp_seq.SDForest","text":"sequence complexity parameters","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the sequence of complexity parameters of an SDForest ‚Äî get_cp_seq.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the sequence of complexity parameters of an SDTree ‚Äî get_cp_seq.SDTree","title":"Get the sequence of complexity parameters of an SDTree ‚Äî get_cp_seq.SDTree","text":"function extracts sequence complexity parameters SDTree result changes tree structure pruned. cp values differ first three digits decimal point returned.","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the sequence of complexity parameters of an SDTree ‚Äî get_cp_seq.SDTree","text":"","code":"# S3 method for class 'SDTree' get_cp_seq(object, ...)"},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the sequence of complexity parameters of an SDTree ‚Äî get_cp_seq.SDTree","text":"object SDTree object ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the sequence of complexity parameters of an SDTree ‚Äî get_cp_seq.SDTree","text":"sequence complexity parameters","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/get_cp_seq.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the sequence of complexity parameters of an SDTree ‚Äî get_cp_seq.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/mergeForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge two forests ‚Äî mergeForest","title":"Merge two forests ‚Äî mergeForest","text":"function merges two forests. trees combined variable importance calculated weighted average two forests. forests trained data, predictions oob_predictions combined well.","code":""},{"path":"https://markusul.github.io/SDForest/reference/mergeForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge two forests ‚Äî mergeForest","text":"","code":"mergeForest(fit1, fit2)"},{"path":"https://markusul.github.io/SDForest/reference/mergeForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge two forests ‚Äî mergeForest","text":"fit1 first SDForest object fit2 second SDForest object","code":""},{"path":"https://markusul.github.io/SDForest/reference/mergeForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge two forests ‚Äî mergeForest","text":"merged SDForest object","code":""},{"path":"https://markusul.github.io/SDForest/reference/mergeForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Merge two forests ‚Äî mergeForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/partDependence.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial dependence ‚Äî partDependence","title":"Partial dependence ‚Äî partDependence","text":"function calculates partial dependence model single variable. predictions made observations dataset varying value variable interest. overall partial effect average predictions. (Friedman 2001)","code":""},{"path":"https://markusul.github.io/SDForest/reference/partDependence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial dependence ‚Äî partDependence","text":"","code":"partDependence(object, j, X = NULL, subSample = NULL, mc.cores = 1)"},{"path":"https://markusul.github.io/SDForest/reference/partDependence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial dependence ‚Äî partDependence","text":"object model object predict method takes newdata argument returns predictions. j variable partial dependence calculated. Either column index variable dataset name variable. X dataset partial dependence calculated. contain variables dataset used train model. NULL, tries extract dataset model object. subSample Number samples draw original data empirical partial dependence. NULL, observations used. mc.cores Number cores use parallel computation. Parallel computing supported unix.","code":""},{"path":"https://markusul.github.io/SDForest/reference/partDependence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial dependence ‚Äî partDependence","text":"object class partDependence containing preds_mean average prediction value variable interest. x_seq sequence values variable interest. preds predictions value variable interest observation. j name variable interest. xj values variable interest dataset.","code":""},{"path":"https://markusul.github.io/SDForest/reference/partDependence.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Partial dependence ‚Äî partDependence","text":"Friedman JH (2001). ‚ÄúGreedy Function Approximation: Gradient Boosting Machine.‚Äù Annals Statistics, 29(5), 1189‚Äì1232. ISSN 00905364, http://www.jstor.org/stable/2699986.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/partDependence.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Partial dependence ‚Äî partDependence","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/partDependence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partial dependence ‚Äî partDependence","text":"","code":"set.seed(1) x <- rnorm(100) y <- sign(x) * 3 + rnorm(100) model <- SDTree(x = x, y = y, Q_type = 'no_deconfounding') pd <- partDependence(model, 1, X = x) plot(pd)"},{"path":"https://markusul.github.io/SDForest/reference/plot.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot SDTree ‚Äî plot.SDTree","title":"Plot SDTree ‚Äî plot.SDTree","text":"Plot SDTree.","code":""},{"path":"https://markusul.github.io/SDForest/reference/plot.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot SDTree ‚Äî plot.SDTree","text":"","code":"# S3 method for class 'SDTree' plot(x, ...)"},{"path":"https://markusul.github.io/SDForest/reference/plot.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot SDTree ‚Äî plot.SDTree","text":"x Fitted object class SDTree. ... arguments passed methods.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/plot.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot SDTree ‚Äî plot.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/plot.partDependence.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot partial dependence ‚Äî plot.partDependence","title":"Plot partial dependence ‚Äî plot.partDependence","text":"function plots partial dependence model single variable.","code":""},{"path":"https://markusul.github.io/SDForest/reference/plot.partDependence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot partial dependence ‚Äî plot.partDependence","text":"","code":"# S3 method for class 'partDependence' plot(x, n_examples = 19, ...)"},{"path":"https://markusul.github.io/SDForest/reference/plot.partDependence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot partial dependence ‚Äî plot.partDependence","text":"x object class partDependence returned partDependence. n_examples Number examples plot addition average prediction. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/plot.partDependence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot partial dependence ‚Äî plot.partDependence","text":"ggplot object.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/plot.partDependence.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot partial dependence ‚Äî plot.partDependence","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/plot.paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the paths of an SDTree or SDForest ‚Äî plot.paths","title":"Visualize the paths of an SDTree or SDForest ‚Äî plot.paths","text":"function visualizes variable importance SDTree SDForest different complexity parameters. regularization path stability selection path can visualized.","code":""},{"path":"https://markusul.github.io/SDForest/reference/plot.paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the paths of an SDTree or SDForest ‚Äî plot.paths","text":"","code":"# S3 method for class 'paths' plot(x, plotly = FALSE, selection = NULL, sqrt_scale = FALSE, ...)"},{"path":"https://markusul.github.io/SDForest/reference/plot.paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the paths of an SDTree or SDForest ‚Äî plot.paths","text":"x paths object plotly TRUE plot returned interactive using plotly. Might slow large data. selection vector indices covariates plotted. Can used plot subset covariates case many covariates. sqrt_scale TRUE y-axis square root scale. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/plot.paths.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the paths of an SDTree or SDForest ‚Äî plot.paths","text":"ggplot object variable importance different regularization. path object includes cp_min value, black dashed line added indicate --bag optimal variable selection.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/plot.paths.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Visualize the paths of an SDTree or SDForest ‚Äî plot.paths","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/plotOOB.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize the out-of-bag performance of an SDForest ‚Äî plotOOB","title":"Visualize the out-of-bag performance of an SDForest ‚Äî plotOOB","text":"function visualizes --bag performance SDForest different complexity parameters. Can used choose optimal complexity parameter.","code":""},{"path":"https://markusul.github.io/SDForest/reference/plotOOB.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize the out-of-bag performance of an SDForest ‚Äî plotOOB","text":"","code":"plotOOB(object, sqrt_scale = FALSE)"},{"path":"https://markusul.github.io/SDForest/reference/plotOOB.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize the out-of-bag performance of an SDForest ‚Äî plotOOB","text":"object paths object loss_path matrix --bag performance complexity parameter. sqrt_scale TRUE x-axis square root scale.","code":""},{"path":"https://markusul.github.io/SDForest/reference/plotOOB.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize the out-of-bag performance of an SDForest ‚Äî plotOOB","text":"ggplot object","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/plotOOB.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Visualize the out-of-bag performance of an SDForest ‚Äî plotOOB","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/predict.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictions for the SDForest ‚Äî predict.SDForest","title":"Predictions for the SDForest ‚Äî predict.SDForest","text":"Predicts response new data using fitted SDForest.","code":""},{"path":"https://markusul.github.io/SDForest/reference/predict.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predictions for the SDForest ‚Äî predict.SDForest","text":"","code":"# S3 method for class 'SDForest' predict(object, newdata, ...)"},{"path":"https://markusul.github.io/SDForest/reference/predict.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predictions for the SDForest ‚Äî predict.SDForest","text":"object Fitted object class SDForest. newdata New test data class data.frame containing covariates predict response. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/predict.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predictions for the SDForest ‚Äî predict.SDForest","text":"vector predictions new data.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/predict.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predictions for the SDForest ‚Äî predict.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/predict.SDForest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predictions for the SDForest ‚Äî predict.SDForest","text":"","code":"set.seed(1) n <- 50 X <- matrix(rnorm(n * 20), nrow = n) y <- sign(X[, 1]) * 3 + rnorm(n) model <- SDForest(x = X, y = y, Q_type = 'no_deconfounding', nTree = 10) predict(model, newdata = data.frame(X)) #>         1         2         3         4         5         6         7         8  #> -2.344572  3.320119 -2.846440  2.025373  2.786994 -3.001182  2.440376  2.719209  #>         9        10        11        12        13        14        15        16  #>  2.126194 -1.895545  2.821547  2.882342 -2.401900 -3.379231  3.166384 -1.965881  #>        17        18        19        20        21        22        23        24  #> -2.613395  1.630138  2.481392  2.837458  1.988028  2.347486  2.055098 -2.104640  #>        25        26        27        28        29        30        31        32  #>  2.872809 -2.321626 -1.951900 -2.336816 -3.037872  1.882414  2.361760 -2.851037  #>        33        34        35        36        37        38        39        40  #>  2.183664 -3.224546 -2.357686 -2.823924 -2.014919 -1.953223  2.561342  2.832949  #>        41        42        43        44        45        46        47        48  #> -2.650911 -2.599645  2.657280  1.676892 -2.289810 -2.317609  2.703465  2.129185  #>        49        50  #> -1.615429  2.948960"},{"path":"https://markusul.github.io/SDForest/reference/predict.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictions for the SDTree ‚Äî predict.SDTree","title":"Predictions for the SDTree ‚Äî predict.SDTree","text":"Predicts response new data using fitted SDTree.","code":""},{"path":"https://markusul.github.io/SDForest/reference/predict.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predictions for the SDTree ‚Äî predict.SDTree","text":"","code":"# S3 method for class 'SDTree' predict(object, newdata, ...)"},{"path":"https://markusul.github.io/SDForest/reference/predict.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predictions for the SDTree ‚Äî predict.SDTree","text":"object Fitted object class SDTree. newdata New test data class data.frame containing covariates predict response. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/predict.SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predictions for the SDTree ‚Äî predict.SDTree","text":"vector predictions new data.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/predict.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predictions for the SDTree ‚Äî predict.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/predict.SDTree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predictions for the SDTree ‚Äî predict.SDTree","text":"","code":"set.seed(1) n <- 50 X <- matrix(rnorm(n * 20), nrow = n) y <- sign(X[, 1]) * 3 + rnorm(n) model <- SDTree(x = X, y = y, Q_type = 'no_deconfounding') predict(model, newdata = data.frame(X)) #>         1         2         3         4         5         6         7         8  #> -2.638193  3.314347 -2.638193  2.379640  2.379640 -3.705198  3.314347  2.379640  #>         9        10        11        12        13        14        15        16  #>  2.379640 -2.638193  3.314347  3.314347 -3.705198 -3.705198  3.314347 -2.638193  #>        17        18        19        20        21        22        23        24  #> -3.705198  2.379640  3.314347  3.314347  3.314347  3.314347  3.314347 -3.705198  #>        25        26        27        28        29        30        31        32  #>  3.314347 -3.705198 -3.705198 -2.638193 -2.638193  3.314347  2.379640 -3.705198  #>        33        34        35        36        37        38        39        40  #>  3.314347 -2.638193 -3.705198 -2.638193 -2.638193 -2.638193  3.314347  2.379640  #>        41        42        43        44        45        46        47        48  #> -2.638193 -2.638193  2.379640  3.314347 -2.638193 -2.638193  3.314347  2.379640  #>        49        50  #> -2.638193  3.314347"},{"path":"https://markusul.github.io/SDForest/reference/predictOOB.html","id":null,"dir":"Reference","previous_headings":"","what":"Out-of-bag predictions for the SDForest ‚Äî predictOOB","title":"Out-of-bag predictions for the SDForest ‚Äî predictOOB","text":"Predicts response training data using trees SDForest trained observation.","code":""},{"path":"https://markusul.github.io/SDForest/reference/predictOOB.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Out-of-bag predictions for the SDForest ‚Äî predictOOB","text":"","code":"predictOOB(object, X = NULL)"},{"path":"https://markusul.github.io/SDForest/reference/predictOOB.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Out-of-bag predictions for the SDForest ‚Äî predictOOB","text":"object Fitted object class SDForest. X Covariates training data. NULL, data saved object used.","code":""},{"path":"https://markusul.github.io/SDForest/reference/predictOOB.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Out-of-bag predictions for the SDForest ‚Äî predictOOB","text":"vector --bag predictions training data.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/predictOOB.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Out-of-bag predictions for the SDForest ‚Äî predictOOB","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/print.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Print SDForest ‚Äî print.SDForest","title":"Print SDForest ‚Äî print.SDForest","text":"Print contents SDForest.","code":""},{"path":"https://markusul.github.io/SDForest/reference/print.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print SDForest ‚Äî print.SDForest","text":"","code":"# S3 method for class 'SDForest' print(x, ...)"},{"path":"https://markusul.github.io/SDForest/reference/print.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print SDForest ‚Äî print.SDForest","text":"x Fitted object class SDForest. ... arguments passed methods.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/print.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print SDForest ‚Äî print.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/print.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a SDTree ‚Äî print.SDTree","title":"Print a SDTree ‚Äî print.SDTree","text":"Print contents SDTree.","code":""},{"path":"https://markusul.github.io/SDForest/reference/print.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a SDTree ‚Äî print.SDTree","text":"","code":"# S3 method for class 'SDTree' print(x, ...)"},{"path":"https://markusul.github.io/SDForest/reference/print.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a SDTree ‚Äî print.SDTree","text":"x Fitted object class SDTree. ... arguments passed methods.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/print.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print a SDTree ‚Äî print.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/prune.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Prune an SDForest ‚Äî prune.SDForest","title":"Prune an SDForest ‚Äî prune.SDForest","text":"Prunes trees forest re-calculates --bag predictions performance measures. training data needed calculate --bag statistics. Note forest pruned place. intend keep original forest, make copy pruning.","code":""},{"path":"https://markusul.github.io/SDForest/reference/prune.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prune an SDForest ‚Äî prune.SDForest","text":"","code":"# S3 method for class 'SDForest' prune(object, cp, X = NULL, Y = NULL, Q = NULL, pred = TRUE, ...)"},{"path":"https://markusul.github.io/SDForest/reference/prune.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prune an SDForest ‚Äî prune.SDForest","text":"object SDForest object cp Complexity parameter, higher value nodes pruned. X training data, NULL data forest object used. Y training response variable, NULL data forest object used. Q transformation matrix, NULL data forest object used. pred TRUE predictions calculated, FALSE --bag statistics calculated. can set FALSE save computation time --bag statistics needed. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/prune.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prune an SDForest ‚Äî prune.SDForest","text":"pruned SDForest object","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/prune.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prune an SDForest ‚Äî prune.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/prune.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Prune an SDTree ‚Äî prune.SDTree","title":"Prune an SDTree ‚Äî prune.SDTree","text":"Removes nodes improve loss cp times initial loss. Either one successors. Note tree pruned place. intend keep original tree, make copy pruning.","code":""},{"path":"https://markusul.github.io/SDForest/reference/prune.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prune an SDTree ‚Äî prune.SDTree","text":"","code":"# S3 method for class 'SDTree' prune(object, cp, ...)"},{"path":"https://markusul.github.io/SDForest/reference/prune.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prune an SDTree ‚Äî prune.SDTree","text":"object SDTree object cp Complexity parameter, higher value nodes pruned. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/prune.SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prune an SDTree ‚Äî prune.SDTree","text":"pruned SDTree object","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/prune.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Prune an SDTree ‚Äî prune.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/prune.SDTree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prune an SDTree ‚Äî prune.SDTree","text":"","code":"set.seed(1) X <- matrix(rnorm(50 * 20), nrow = 50) Y <- rnorm(50) tree <- SDTree(x = X, y = Y) pruned_tree <- prune(copy(tree), 0.2) tree #>                    levelName        value           s  j       label decision #> 1  1                         -0.023180294  1.14111763 19 X19 <= 1.14          #> 2   ¬¶--1                     -0.141832617  0.34537161  3  X3 <= 0.35       no #> 3   ¬¶   ¬¶--1                  0.059413265 -0.77654438  6 X6 <= -0.78       no #> 4   ¬¶   ¬¶   ¬¶--1             -0.829336723          NA NA        -0.8       no #> 5   ¬¶   ¬¶   ¬∞--4              0.209165300  0.55785195 11 X11 <= 0.56      yes #> 6   ¬¶   ¬¶       ¬¶--4         -0.007902778  0.33514911 20 X20 <= 0.34       no #> 7   ¬¶   ¬¶       ¬¶   ¬¶--4      0.277097572  0.26853017 17 X17 <= 0.27       no #> 8   ¬¶   ¬¶       ¬¶   ¬¶   ¬¶--4  0.629359018          NA NA         0.6       no #> 9   ¬¶   ¬¶       ¬¶   ¬¶   ¬∞--8 -0.121703564          NA NA        -0.1      yes #> 10  ¬¶   ¬¶       ¬¶   ¬∞--7     -0.436583926          NA NA        -0.4      yes #> 11  ¬¶   ¬¶       ¬∞--6          0.900286010          NA NA         0.9      yes #> 12  ¬¶   ¬∞--3                 -0.835357400  0.09259939  4  X4 <= 0.09      yes #> 13  ¬¶       ¬¶--3             -1.453742836          NA NA        -1.5       no #> 14  ¬¶       ¬∞--5             -0.152590731          NA NA        -0.2      yes #> 15  ¬∞--2                      0.828549713          NA NA         0.8      yes #>    n_samples #> 1         50 #> 2         44 #> 3         34 #> 4          5 #> 5         29 #> 6         22 #> 7         13 #> 8          7 #> 9          6 #> 10         9 #> 11         7 #> 12        10 #> 13         5 #> 14         5 #> 15         6 pruned_tree #>   levelName       value        s  j label decision n_samples #> 1         1 -0.02318029 1.141118 19     0       NA        50"},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the regularization path of an SDForest ‚Äî regPath.SDForest","title":"Calculate the regularization path of an SDForest ‚Äî regPath.SDForest","text":"function calculates variable importance SDForest --bag performance different complexity parameters.","code":""},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the regularization path of an SDForest ‚Äî regPath.SDForest","text":"","code":"# S3 method for class 'SDForest' regPath(object, cp_seq = NULL, X = NULL, Y = NULL, Q = NULL, copy = TRUE, ...)"},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the regularization path of an SDForest ‚Äî regPath.SDForest","text":"object SDForest object cp_seq sequence complexity parameters. NULL, sequence calculated automatically using relevant values. X training data, NULL data forest object used. Y training response variable, NULL data forest object used. Q transformation matrix, NULL data forest object used. copy Whether tree copied regularization path. FALSE, pruning done place change SDForest. might reasonable, SDForest large copy. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the regularization path of an SDForest ‚Äî regPath.SDForest","text":"object class paths containing cp sequence complexity parameters. varImp_path matrix variable importance complexity parameter. loss_path matrix --bag performance complexity parameter. cp_min complexity parameter lowest --bag performance.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate the regularization path of an SDForest ‚Äî regPath.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDForest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the regularization path of an SDForest ‚Äî regPath.SDForest","text":"","code":"set.seed(1) n <- 10 X <- matrix(rnorm(n * 5), nrow = n) y <- sign(X[, 1]) * 3 + sign(X[, 2]) + rnorm(n) model <- SDForest(x = X, y = y, Q_type = 'no_deconfounding') paths <- regPath(model) plotOOB(paths)  plot(paths)  if (FALSE) { # \\dontrun{ plot(paths, plotly = TRUE) } # }"},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the regularization path of an SDTree ‚Äî regPath.SDTree","title":"Calculate the regularization path of an SDTree ‚Äî regPath.SDTree","text":"function calculates variable importance SDTree different complexity parameters.","code":""},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the regularization path of an SDTree ‚Äî regPath.SDTree","text":"","code":"# S3 method for class 'SDTree' regPath(object, cp_seq = NULL, ...)"},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the regularization path of an SDTree ‚Äî regPath.SDTree","text":"object SDTree object cp_seq sequence complexity parameters. NULL, sequence calculated automatically using relevant values. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the regularization path of an SDTree ‚Äî regPath.SDTree","text":"object class paths containing cp sequence complexity parameters. varImp_path matrix variable importance complexity parameter.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate the regularization path of an SDTree ‚Äî regPath.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/regPath.SDTree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the regularization path of an SDTree ‚Äî regPath.SDTree","text":"","code":"set.seed(1) n <- 10 X <- matrix(rnorm(n * 5), nrow = n) y <- sign(X[, 1]) * 3 + sign(X[, 2]) + rnorm(n) model <- SDTree(x = X, y = y, Q_type = 'no_deconfounding') paths <- regPath(model) plot(paths)  if (FALSE) { # \\dontrun{ plot(paths, plotly = TRUE) } # }"},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_nonlinear.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate data with linear confounding and non-linear causal effect ‚Äî simulate_data_nonlinear","title":"Simulate data with linear confounding and non-linear causal effect ‚Äî simulate_data_nonlinear","text":"Simulation data confounded non-linear model. data generating process given : $$Y = f(X) + H \\delta + \\nu$$ $$X = H \\Gamma + E$$ \\(f(X)\\) random function fourier basis subset size m covariates \\(X_j\\) causal effect \\(Y\\). $$f(x_i) = \\sum_{j = 1}^p 1_{j \\js} \\sum_{k = 1}^K (\\beta_{j, k}^{(1)} \\cos(0.2 k x_j) + \\beta_{j, k}^{(2)} \\sin(0.2 k x_j))$$ \\(E\\), \\(\\nu\\) random error terms \\(H \\\\mathbb{R}^{n \\times q}\\) matrix random confounding covariates. \\(\\Gamma \\\\mathbb{R}^{q \\times p}\\) \\(\\delta \\\\mathbb{R}^{q}\\) random coefficient vectors. simulation, parameters drawn standard normal distribution, except \\(\\nu\\) drawn normal distribution standard deviation 0.1. parameters \\(\\beta\\) drawn uniform distribution -1 1.","code":""},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_nonlinear.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate data with linear confounding and non-linear causal effect ‚Äî simulate_data_nonlinear","text":"","code":"simulate_data_nonlinear(q, p, n, m, K = 2, eff = NULL, fixEff = FALSE)"},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_nonlinear.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate data with linear confounding and non-linear causal effect ‚Äî simulate_data_nonlinear","text":"q number confounding covariates H p number covariates X n number observations m number covariates causal effect Y K number fourier basis functions K \\(K \\\\mathbb{N}\\), e.g. complexity causal function eff number affected covariates X confounding, NULL covariates affected fixEff eff smaller p: fixEff = TRUE, causal parents always affected confounding fixEff = FALSE, affected covariates chosen completely random.","code":""},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_nonlinear.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate data with linear confounding and non-linear causal effect ‚Äî simulate_data_nonlinear","text":"list containing simulated data: X matrix covariates Y vector responses f_X vector true function f(X) j indices causal covariates X beta parameter vector function f(X), see f_four H matrix confounding covariates","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_nonlinear.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate data with linear confounding and non-linear causal effect ‚Äî simulate_data_nonlinear","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_step.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate data with linear confounding and causal effect following a step-function ‚Äî simulate_data_step","title":"Simulate data with linear confounding and causal effect following a step-function ‚Äî simulate_data_step","text":"Simulation data confounded non-linear model. non-linear function random regression tree. data generating process given : $$Y = f(X) + H \\delta + \\nu$$ $$X = H \\Gamma + E$$ \\(f(X)\\) random regression tree \\(m\\) random splits data. Resulting random step-function \\(m+1\\) levels, .e. leaf-levels. $$f(x_i) = \\sum_{k = 1}^K 1_{\\{x_i \\R_k\\}} c_k$$ \\(E\\), \\(\\nu\\) random error terms \\(H \\\\mathbb{R}^{n \\times q}\\) matrix random confounding covariates. \\(\\Gamma \\\\mathbb{R}^{q \\times p}\\) \\(\\delta \\\\mathbb{R}^{q}\\) random coefficient vectors. simulation, parameters drawn standard normal distribution, except \\(\\delta\\) drawn normal distribution standard deviation 10. leaf levels \\(c_k\\) drawn uniform distribution -50 50.","code":""},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_step.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate data with linear confounding and causal effect following a step-function ‚Äî simulate_data_step","text":"","code":"simulate_data_step(q, p, n, m, make_tree = FALSE)"},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_step.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate data with linear confounding and causal effect following a step-function ‚Äî simulate_data_step","text":"q number confounding covariates H p number covariates X n number observations m number covariates causal effect Y make_tree Whether random regression tree returned.","code":""},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_step.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate data with linear confounding and causal effect following a step-function ‚Äî simulate_data_step","text":"list containing simulated data: X matrix covariates Y vector responses f_X vector true function f(X) j indices causal covariates X tree make_tree, random regression tree class Node (Glur 2023)","code":""},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_step.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulate data with linear confounding and causal effect following a step-function ‚Äî simulate_data_step","text":"Glur C (2023). ‚Äúdata.tree: General Purpose Hierarchical Data Structure.‚Äù https://CRAN.R-project.org/package=data.tree.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/simulate_data_step.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Simulate data with linear confounding and causal effect following a step-function ‚Äî simulate_data_step","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/stabilitySelection.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the stability selection of an SDForest ‚Äî stabilitySelection.SDForest","title":"Calculate the stability selection of an SDForest ‚Äî stabilitySelection.SDForest","text":"function calculates stability selection SDForest (Meinshausen B√ºhlmann 2010) . Stability selection calculated fraction trees forest select variable split complexity parameter.","code":""},{"path":"https://markusul.github.io/SDForest/reference/stabilitySelection.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the stability selection of an SDForest ‚Äî stabilitySelection.SDForest","text":"","code":"# S3 method for class 'SDForest' stabilitySelection(object, cp_seq = NULL, ...)"},{"path":"https://markusul.github.io/SDForest/reference/stabilitySelection.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the stability selection of an SDForest ‚Äî stabilitySelection.SDForest","text":"object SDForest object cp_seq sequence complexity parameters. NULL, sequence calculated automatically using relevant values. ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/stabilitySelection.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the stability selection of an SDForest ‚Äî stabilitySelection.SDForest","text":"object class paths containing cp sequence complexity parameters. varImp_path matrix stability selection complexity parameter.","code":""},{"path":"https://markusul.github.io/SDForest/reference/stabilitySelection.SDForest.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate the stability selection of an SDForest ‚Äî stabilitySelection.SDForest","text":"Meinshausen N, B√ºhlmann P (2010). ‚ÄúStability Selection.‚Äù Journal Royal Statistical Society Series B: Statistical Methodology, 72(4), 417‚Äì473. ISSN 1369-7412, doi:10.1111/j.1467-9868.2010.00740.x .","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/stabilitySelection.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate the stability selection of an SDForest ‚Äî stabilitySelection.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/stabilitySelection.SDForest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the stability selection of an SDForest ‚Äî stabilitySelection.SDForest","text":"","code":"set.seed(1) n <- 10 X <- matrix(rnorm(n * 5), nrow = n) y <- sign(X[, 1]) * 3 + sign(X[, 2]) + rnorm(n) model <- SDForest(x = X, y = y, Q_type = 'no_deconfounding') paths <- stabilitySelection(model) plot(paths)  if (FALSE) { # \\dontrun{ plot(paths, plotly = TRUE) } # }"},{"path":"https://markusul.github.io/SDForest/reference/toList.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"SDForest toList method ‚Äî toList.SDForest","title":"SDForest toList method ‚Äî toList.SDForest","text":"Converts trees SDForest object class Node (Glur 2023)  class list. makes substantially easier save forest disk.","code":""},{"path":"https://markusul.github.io/SDForest/reference/toList.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SDForest toList method ‚Äî toList.SDForest","text":"","code":"# S3 method for class 'SDForest' toList(object, ...)"},{"path":"https://markusul.github.io/SDForest/reference/toList.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SDForest toList method ‚Äî toList.SDForest","text":"object SDForest object trees Node format ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/toList.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SDForest toList method ‚Äî toList.SDForest","text":"SDForest object trees list format","code":""},{"path":"https://markusul.github.io/SDForest/reference/toList.SDForest.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SDForest toList method ‚Äî toList.SDForest","text":"Glur C (2023). ‚Äúdata.tree: General Purpose Hierarchical Data Structure.‚Äù https://CRAN.R-project.org/package=data.tree.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/toList.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SDForest toList method ‚Äî toList.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/toList.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"SDTree toList method ‚Äî toList.SDTree","title":"SDTree toList method ‚Äî toList.SDTree","text":"Converts tree SDTree object class Node (Glur 2023)  class list. makes substantially easier save tree disk.","code":""},{"path":"https://markusul.github.io/SDForest/reference/toList.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SDTree toList method ‚Äî toList.SDTree","text":"","code":"# S3 method for class 'SDTree' toList(object, ...)"},{"path":"https://markusul.github.io/SDForest/reference/toList.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SDTree toList method ‚Äî toList.SDTree","text":"object SDTree object tree Node format ... arguments passed methods.","code":""},{"path":"https://markusul.github.io/SDForest/reference/toList.SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SDTree toList method ‚Äî toList.SDTree","text":"SDTree object tree list format","code":""},{"path":"https://markusul.github.io/SDForest/reference/toList.SDTree.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"SDTree toList method ‚Äî toList.SDTree","text":"Glur C (2023). ‚Äúdata.tree: General Purpose Hierarchical Data Structure.‚Äù https://CRAN.R-project.org/package=data.tree.","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/toList.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"SDTree toList method ‚Äî toList.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract variable importance of an SDForest ‚Äî varImp.SDForest","title":"Extract variable importance of an SDForest ‚Äî varImp.SDForest","text":"function extracts variable importance SDForest. variable importance calculated sum decrease loss function resulting splits use covariate tree. mean variable importance trees results variable importance forest.","code":""},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract variable importance of an SDForest ‚Äî varImp.SDForest","text":"","code":"# S3 method for class 'SDForest' varImp(object)"},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract variable importance of an SDForest ‚Äî varImp.SDForest","text":"object SDForest object","code":""},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract variable importance of an SDForest ‚Äî varImp.SDForest","text":"named vector variable importance","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDForest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract variable importance of an SDForest ‚Äî varImp.SDForest","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDForest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract variable importance of an SDForest ‚Äî varImp.SDForest","text":"","code":"data(iris) fit <- SDForest(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,                   iris, nTree = 10) varImp(fit) #>  Sepal.Width Petal.Length  Petal.Width  #>   0.05903544   0.15647276   0.12058270"},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract variable importance of an SDTree ‚Äî varImp.SDTree","title":"Extract variable importance of an SDTree ‚Äî varImp.SDTree","text":"function extracts variable importance SDTree. variable importance calculated sum decrease loss function resulting splits use covariate.","code":""},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract variable importance of an SDTree ‚Äî varImp.SDTree","text":"","code":"# S3 method for class 'SDTree' varImp(object)"},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract variable importance of an SDTree ‚Äî varImp.SDTree","text":"object SDTree object","code":""},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract variable importance of an SDTree ‚Äî varImp.SDTree","text":"named vector variable importance","code":""},{"path":[]},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDTree.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract variable importance of an SDTree ‚Äî varImp.SDTree","text":"Markus Ulmer","code":""},{"path":"https://markusul.github.io/SDForest/reference/varImp.SDTree.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract variable importance of an SDTree ‚Äî varImp.SDTree","text":"","code":"data(iris) tree <- SDTree(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, iris) varImp(tree) #>  Sepal.Width Petal.Length  Petal.Width  #>    0.0247580    0.3026671    0.0000000"}]
